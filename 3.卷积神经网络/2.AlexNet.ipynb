{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import cifar10_input\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "这是一个包含60000张32×3232×32图片的数据库, 包含50000张训练集和10000张测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:233: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:82: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:129: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:135: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64\n",
    "\n",
    "#  数据集地址\n",
    "data_dir = 'E:/data/cifar10_data/cifar-10-batches-bin/'\n",
    "\n",
    "# 获取训练集\n",
    "train_imgs, train_labels = cifar10_input.inputs(eval_data=False,\n",
    "                                                data_dir=data_dir,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "# 获取测试集\n",
    "test_imgs, test_labels = cifar10_input.inputs(eval_data=True,\n",
    "                                              data_dir=data_dir,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数：50000\n",
      "测试集样本数：10000\n"
     ]
    }
   ],
   "source": [
    "train_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN# 训练样本的个数\n",
    "test_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL# 测试样本的个数\n",
    "print('训练集样本数：%d'%train_examples)\n",
    "print('测试集样本数：%d'%test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造权重变量\n",
    "def variable_weight(shape, stddev=5e-2):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='weight')\n",
    "\n",
    "# 构造偏置变量\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(initializer=init, shape=shape, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, ksize, out_depth, strides, padding='SAME', act=tf.nn.relu,\n",
    "         scope='conv_layer', reuse=None):\n",
    "    \"\"\"构造一个卷积层    \n",
    "    Args:        \n",
    "        x: 输入        \n",
    "        ksize: 卷积核的大小, 一个⻓度为2的`list`, 例如[3, 3]        \n",
    "        output_depth: 卷积核的个数        \n",
    "        strides: 卷积核移动的步⻓, 一个⻓度为2的`list`, 例如[2, 2]        \n",
    "        padding: 卷积核的补0策略        \n",
    "        act: 完成卷积后的激活函数, 默认是`tf.nn.relu`        \n",
    "        scope: 这一层的名称(可选)        \n",
    "        reuse: 是否复用   \n",
    "    Return:        \n",
    "        out: 卷积层的结果    \n",
    "    \"\"\"\n",
    "    # 这里默认数据是NHWC输入的\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 先构造卷积核\n",
    "        kernel_shape = ksize + [in_depth, out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(kernel_shape)\n",
    "        \n",
    "        strides = [1,strides[0], strides[1], 1]\n",
    "        # 生成卷积\n",
    "        conv = tf.nn.conv2d(input=x, filter=kernel, strides=strides,\n",
    "                            padding=padding, name='conv')\n",
    "        \n",
    "        # 构造偏置\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        \n",
    "        # 和偏置相加\n",
    "        preact = tf.nn.bias_add(conv, bias)\n",
    "        \n",
    "        # 添加激活层\n",
    "        out = act(preact)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, ksize, strides, padding='SAME', name='pool_layer'):\n",
    "    \"\"\"构造一个最大值池化层    \n",
    "    Args:        \n",
    "        x: 输入       \n",
    "        ksize: pooling核的大小, 一个⻓度为2的`list`, 例如[3, 3]        \n",
    "        strides: pooling核移动的步⻓, 一个⻓度为2的`list`, 例如[2, 2]        \n",
    "        padding: pooling的补0策略        \n",
    "        name: 这一层的名称(可选)    \n",
    "    Return:        \n",
    "        pooling层的结果    \n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(value=x, ksize=[1, ksize[0], ksize[1], 1],\n",
    "                          strides=[1, strides[0], strides[1], 1], padding=padding, name=name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    \"\"\"构造一个全连接层    \n",
    "    Args:        \n",
    "        x: 输入        \n",
    "        out_depth: 输出向量的维数        \n",
    "        act: 激活函数, 默认是`tf.nn.relu`        \n",
    "        scope: 名称域, 默认是`fully_connect`        \n",
    "        reuse: 是否需要重用   \n",
    "    \"\"\" \n",
    "    # 获取输入层通道数\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    # 构造全连接层的参数\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 构造权重\n",
    "        with tf.variable_scope('weight'):\n",
    "            weight = variable_weight([in_depth,out_depth])\n",
    "        # 构造偏置项\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        # 一个线性函数\n",
    "        fc = tf.nn.bias_add(tf.matmul(x, weight), bias)\n",
    "        # 激活函数作用\n",
    "        out = act(fc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AlexNet(inputs, reuse=None):\n",
    "    \"\"\"构建 Alexnet 的前向传播    \n",
    "    Args:        \n",
    "        inpus: 输入        \n",
    "        reuse: 是否需要重用    \n",
    "    Return:        \n",
    "        net: alexnet的结果    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        \n",
    "        net = conv(inputs, [5,5], 64, [1,1], padding='VALID', scope='conv1')\n",
    "        \n",
    "        \n",
    "        net = max_pool(net, [3,3], [2,2], padding='VALID', name='pool1')\n",
    "        \n",
    "        \n",
    "        net = conv(net, [5,5], 64, [1,1], scope='conv2')\n",
    "        \n",
    "        \n",
    "        net = max_pool(net, [3,3], [2,2], padding='VALID', name='pool2')\n",
    "        \n",
    "        \n",
    "        net = tf.reshape(net, [-1, 6*6*64])\n",
    "        \n",
    "        \n",
    "        net = fc(net, 384, scope='fc3')\n",
    "        \n",
    "        \n",
    "        net = fc(net, 192, scope='fc4')\n",
    "        \n",
    "        \n",
    "        net = fc(net, 10, scope='fc5', act=tf.identity)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_out = AlexNet(train_imgs)\n",
    "test_out = AlexNet(test_imgs, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, \n",
    "                                                        logits=train_out, scope='train')\n",
    "    test_loss = tf.losses.sparse_softmax_cross_entropy(labels=test_labels,\n",
    "                                                       logits=test_out, scope='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1,output_type=tf.int32),\n",
    "                                                    train_labels), tf.float32))\n",
    "    with tf.name_scope('train'):\n",
    "        test_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_out, axis=-1,output_type=tf.int32),\n",
    "                                                       test_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造训练op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "BATCH_SIZE=64\n",
    "def train(train_op, train_loss, train_acc, \n",
    "          val_loss, val_acc, \n",
    "          max_step, batch_size=BATCH_SIZE,\n",
    "          train_log_step=1000, val_log_step=4000):\n",
    "    \"\"\"训练函数\n",
    "      Args:\n",
    "        train_op: 训练`op`\n",
    "        train_loss: 训练集计算误差的`op`\n",
    "        train_acc: 训练集计算正确率的`op`\n",
    "        val_loss: 验证集计算误差的`op`\n",
    "        val_acc: 验证集计算正确率的`op`\n",
    "        max_step: 最大迭代步长\n",
    "        batch_sise: 一个批次中样本的个数\n",
    "        train_log_step: 每隔多少步进行一次训练集信息输出\n",
    "        val_log_step: 每隔多少步进行一次验证集信息输出\n",
    "    \n",
    "      Return:\n",
    "        None\n",
    "      \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "        try:\n",
    "            _start = time.time()\n",
    "            for step in range(max_step + 1):\n",
    "                sess.run(train_op)\n",
    "\n",
    "                if step % train_log_step == 0:\n",
    "                    _end = time.time()\n",
    "                    duration = _end - _start\n",
    "\n",
    "                    train_loss_, train_acc_ = sess.run([train_loss, train_acc])\n",
    "\n",
    "                    sec_per_batch = 1.0 * duration / train_log_step\n",
    "\n",
    "                    print('[train]: step %d loss = %.4f acc = %.4f (%.4f / batch)' \\\n",
    "                      % (step, train_loss_, train_acc_, sec_per_batch))\n",
    "\n",
    "                    _start = _end\n",
    "\n",
    "                if step % val_log_step == 0:\n",
    "                    val_loss_, val_acc_ = sess.run([val_loss, val_acc])\n",
    "                    print('[val]: step %d loss = %.4f acc = %.4f' % (step, val_loss_, val_acc_))\n",
    "\n",
    "            print('-------------------------Over all Result-------------------------')\n",
    "\n",
    "            train_loss_, train_acc_ = _evaluation_no_bn(sess, train_loss, train_acc, NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN, batch_size)\n",
    "            print('[TRAIN]: loss = %.4f acc = %.4f' % (train_loss_, train_acc_))\n",
    "\n",
    "            val_loss_, val_acc_ = _evaluation_no_bn(sess, val_loss, val_acc, NUM_EXAMPLES_PER_EPOCH_FOR_EVAL, batch_size)\n",
    "            print('[VAL]: loss = %.4f acc = %.4f' % (val_loss_, val_acc_))\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Epoch Limited. Done!')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "def _evaluation_no_bn(sess, loss_op, acc_op, num_examples, batch_size):\n",
    "    max_steps = num_examples // batch_size\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for _ in range(max_steps):\n",
    "        loss_value, acc_value = sess.run([loss_op, acc_op])\n",
    "        losses.append(loss_value)\n",
    "        accs.append(acc_value)\n",
    "\n",
    "    mean_loss = np.array(losses, dtype=np.float32).mean()\n",
    "    mean_acc = np.array(accs, dtype=np.float32).mean()\n",
    "\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.3299 acc = 0.1406 (0.0083 / batch)\n",
      "[val]: step 0 loss = 2.4330 acc = 0.0469\n",
      "[train]: step 1000 loss = 1.1107 acc = 0.6875 (0.0402 / batch)\n",
      "[train]: step 2000 loss = 0.7037 acc = 0.7344 (0.0411 / batch)\n",
      "[train]: step 3000 loss = 0.8154 acc = 0.7344 (0.0437 / batch)\n",
      "[train]: step 4000 loss = 0.2628 acc = 0.9062 (0.0493 / batch)\n",
      "[val]: step 4000 loss = 0.8344 acc = 0.7344\n",
      "[train]: step 5000 loss = 0.4574 acc = 0.8281 (0.0455 / batch)\n",
      "[train]: step 6000 loss = 0.4139 acc = 0.8438 (0.0443 / batch)\n",
      "[train]: step 7000 loss = 0.2401 acc = 0.9062 (0.0463 / batch)\n",
      "[train]: step 8000 loss = 0.2192 acc = 0.9219 (0.0462 / batch)\n",
      "[val]: step 8000 loss = 1.1674 acc = 0.7969\n",
      "[train]: step 9000 loss = 0.0820 acc = 0.9844 (0.0450 / batch)\n",
      "[train]: step 10000 loss = 0.2050 acc = 0.9062 (0.0449 / batch)\n",
      "[train]: step 11000 loss = 0.2135 acc = 0.9219 (0.0447 / batch)\n",
      "[train]: step 12000 loss = 0.1096 acc = 0.9375 (0.0454 / batch)\n",
      "[val]: step 12000 loss = 1.6042 acc = 0.6094\n",
      "[train]: step 13000 loss = 0.0084 acc = 1.0000 (0.0502 / batch)\n",
      "[train]: step 14000 loss = 0.0922 acc = 0.9531 (0.0459 / batch)\n",
      "[train]: step 15000 loss = 0.1220 acc = 0.9688 (0.0466 / batch)\n",
      "[train]: step 16000 loss = 0.1591 acc = 0.9375 (0.0469 / batch)\n",
      "[val]: step 16000 loss = 3.5980 acc = 0.6094\n"
     ]
    }
   ],
   "source": [
    "train(train_op, train_loss, train_acc, test_loss, test_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
