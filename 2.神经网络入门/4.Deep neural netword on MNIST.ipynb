{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST手写体识别\n",
    "MNIST手写体识别是图像识别中最经典的问题, 希望能够识别出人类手写的数字. 数据是65000张灰度图和对应的数字. 我们用之前的深度神经网络来尝试解决这个问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们导入mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f5eab42c3679>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\data\\MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\data\\MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting E:\\data\\MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\data\\MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('E:\\\\data\\\\MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到read_data_sets的一个参数是one_hot<br>\n",
    "这个数据集分成了两个部分: 训练和测试. 分开来是为了观察模型在完全没⻅过的数据上的表现, 从而体现泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAC2CAYAAAACw1DjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD/INuTgm4HrkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=6, nrows=2)\n",
    "plt.tight_layout(w_pad=-2.0, h_pad=-10.0)\n",
    "\n",
    "# 调用next_batch方法来一次性获取12个样本\n",
    "# 这里有一个`shuffle`参数, 表达是否打乱样本间的顺序\n",
    "images, labels = train_set.next_batch(12, shuffle=False)\n",
    "\n",
    "for ind,(image, label) in enumerate(zip(images, labels)):\n",
    "    # image 是一个 784 维的向量, 是图片进行拉伸产生的, 这里我们给它 reshape 回去\n",
    "    image = image.reshape((28,28))\n",
    "    \n",
    "    # label 是一个 10 维的向量, 哪个下标处的值为1 说明是数字几\n",
    "    label = label.argmax()\n",
    "    \n",
    "    row = ind // 6  \n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image, cmap='gray')\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d'%label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义深度网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义隐藏层\n",
    "def hidden_layer(layer_input, output_depth, scope='hidden_layer', reuse=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 注意这里的初始化方法是truncated_normal\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                           shape=(input_depth,output_depth), name='weights')\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1),\n",
    "                            shape=(output_depth), name='bias')\n",
    "        net = tf.matmul(layer_input,w)+b\n",
    "        return net\n",
    "    \n",
    "#定义网络结构\n",
    "def DNN(x,output_depths, scope='DNN', reuse=None):\n",
    "    net = x\n",
    "    for i, output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net, output_depth, scope='layer%d'%(i+1), reuse=reuse)\n",
    "        # 注意这里的激活函数\n",
    "        net = tf.nn.relu(net)\n",
    "        # 数字分为0, 1, ..., 9 所以这是10分类问题\n",
    "        # 对应于 one_hot 的标签, 所以这里输出一个 10维的向量\n",
    "    net = hidden_layer(net, 10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "input_ph = tf.placeholder(dtype=tf.float32, shape=(None,784))\n",
    "label_ph = tf.placeholder(dtype=tf.int64, shape=(None,10))\n",
    "\n",
    "# 构造一个4层的神经网络, 它的隐藏节点数分别为: 400, 200, 100, 10\n",
    "dnn = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这是一个分类问题, 因此我们采用交叉熵来计算损失函数\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "# 下面定义的是正确率, 注意理解它为什么是这么定义的\n",
    "accuracy = tf.reduce_mean(tf.cast(\n",
    "                 tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), \n",
    "                 dtype=tf.float32))\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000  Train_loss 0.5131  Test_accuracy 0.875\n",
      "Epoch 2000  Train_loss 0.2638  Test_accuracy 0.891\n",
      "Epoch 3000  Train_loss 0.1911  Test_accuracy 0.938\n",
      "Epoch 4000  Train_loss 0.1218  Test_accuracy 0.969\n",
      "Epoch 5000  Train_loss 0.0736  Test_accuracy 1.000\n",
      "Epoch 6000  Train_loss 0.0871  Test_accuracy 0.953\n",
      "Epoch 7000  Train_loss 0.2009  Test_accuracy 0.906\n",
      "Epoch 8000  Train_loss 0.1032  Test_accuracy 0.984\n",
      "Epoch 9000  Train_loss 0.0481  Test_accuracy 0.984\n",
      "Epoch 10000  Train_loss 0.0828  Test_accuracy 0.969\n",
      "Epoch 11000  Train_loss 0.0897  Test_accuracy 0.969\n",
      "Epoch 12000  Train_loss 0.1096  Test_accuracy 0.984\n",
      "Epoch 13000  Train_loss 0.0355  Test_accuracy 1.000\n",
      "Epoch 14000  Train_loss 0.0581  Test_accuracy 0.984\n",
      "Epoch 15000  Train_loss 0.0444  Test_accuracy 1.000\n",
      "Epoch 16000  Train_loss 0.0337  Test_accuracy 1.000\n",
      "Epoch 17000  Train_loss 0.0374  Test_accuracy 0.984\n",
      "Epoch 18000  Train_loss 0.0267  Test_accuracy 1.000\n",
      "Epoch 19000  Train_loss 0.0289  Test_accuracy 1.000\n",
      "Epoch 20000  Train_loss 0.0083  Test_accuracy 1.000\n",
      "\n",
      "Train Finished!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(20000):\n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if (i+1)%1000 ==0:\n",
    "        # 获取 batch_size 个测试样本\n",
    "        test_imgs, test_labels = train_set.next_batch(batch_size)\n",
    "        # 计算在当前样本上的训练以及测试样本的损失值和正确率\n",
    "        loss_test, acc_test = sess.run([loss,accuracy], feed_dict={input_ph:test_imgs, label_ph:test_labels})\n",
    "        print('Epoch %d  Train_loss %.4f  Test_accuracy %.3f'%(i+1, loss_test, acc_test))\n",
    "print('\\nTrain Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:0.0508\n",
      "Train Accuracy:0.986\n",
      "\n",
      "Test Loss:0.0942\n",
      "Test Accuracy:0.970\n"
     ]
    }
   ],
   "source": [
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "batch_size = 64\n",
    "for i in range(train_set.num_examples // batch_size):\n",
    "    images, labels = train_set.next_batch(batch_size=batch_size)\n",
    "    loss_train, acc_train = sess.run([loss, accuracy], \n",
    "                                     feed_dict={input_ph:images, label_ph:labels})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "print('Train Loss:{:.4f}'.format(np.array(train_loss).mean()))\n",
    "print('Train Accuracy:{:.3f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "batch_size = 64\n",
    "for i in range(test_set.num_examples // batch_size):\n",
    "    images, labels = test_set.next_batch(batch_size=batch_size)\n",
    "    loss_test, acc_test = sess.run([loss, accuracy], \n",
    "                                     feed_dict={input_ph:images, label_ph:labels})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "print('\\nTest Loss:{:.4f}'.format(np.array(test_loss).mean()))\n",
    "print('Test Accuracy:{:.3f}'.format(np.array(test_acc).mean()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard & tf.summary\n",
    "已经给大家分享过如何使用Tensorboard将我们构建的计算图显示出来, 这里我们还要介绍它和tf.summary结合起来体现的更加强大的功能: 可视化训练.首先介绍一下tf.summary, 它能够收集训练过程中的各种tensor的信息并把它保存起来以供Tensorboard读取并展示. 按照下面的方法来使用它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造summary\n",
    "- 如果你想收集表示一个标量或者一个数的tensor的信息, 比如上面的loss<br>\n",
    "`loss_sum = tf.summary.scalar('loss', loss)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的语句就会告诉Tensorflow, 在运行过程中, 我要让Tensorboard显示误差的变化了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你想收集一个tensor的分布情况, 这个tensor可以是任意形状, 比如我们定义了一个(784, 400)的权重w<br>`w_hist = tf.summary.histogram('w_hist', w)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你想收集一个4维的1-通道(灰度图), 3-通道(RGB), 4-通道(RGBA)的tensor的变化, 比如我们输出了一个(1, 8, 8, 1)的灰度图image\n",
    "<br> `image_sum = tf.summary.image('image', image)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你想收集一个3维(batch, frame, channel), 2维(batch, frame)的变化, 比如我们输出了一个(10, 50,3)的tensor:audio<br>\n",
    "`audio_sum = tf.summary.audio('audio', audio)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这次课程中, 我们暂时先使用scalar和histogram的summary, image和audio的summary将在之后的课程中介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重置计算图\n",
    "# 调用此函数后使用任何以前创建的tf.Operation或tf.Tensor对象将导致未定义的行为\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# 重新定义占位符\n",
    "input_ph = tf.placeholder(shape=(None,784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None,10), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 我们需要重新构建前向神经网络, 为了简化代码, 我们在构造一个隐藏层以及它的参数的函数内部构造tf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造权重, 用`truncated_normal`初始化\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造偏置, 用`0.1`初始化\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造添加`variable`的`summary`的函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算平均值\n",
    "        mean = tf.reduce_mean(var)\n",
    "        # 将平均值添加到`summary`中, 这是一个数值, 所以我们用`tf.summary.scalar`\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算标准差\n",
    "        with tf.name_scope('sttdev'):\n",
    "            sttdev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "            # 将标准差添加到`summary`中\n",
    "            tf.summary.scalar('sttdev', sttdev)\n",
    "\n",
    "        # 添加最大值,最小值`summary`\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "\n",
    "        # 添加这个变量分布情况的`summary`, 我们希望观察它的分布, 所以用`tf.summary.histogram`\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造一个隐藏层\n",
    "def hidden_layer(x, output_dim, scope='hidden_layer', act=tf.nn.relu, reuse=None):\n",
    "    # 获取输入的‘depth’\n",
    "    input_dim = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            # 构造`weight`\n",
    "            weight = weight_variable([input_dim,output_dim])\n",
    "            # 添加`weight`的`summary`\n",
    "            variable_summaries(weight)\n",
    "        with tf.name_scope('bias'):\n",
    "            # 构造`bias`\n",
    "            bias = bias_variable([output_dim])\n",
    "            # 添加`bias`的`summary`\n",
    "            variable_summaries(bias)\n",
    "        \n",
    "        with tf.name_scope('linear'):\n",
    "            # 计算`xw+b`\n",
    "            preact = tf.matmul(x,weight)+bias\n",
    "            # 添加激活层之前输出的分布情况到`summary`\n",
    "            tf.summary.histogram('pre_activation', preact)\n",
    "        \n",
    "        # 经过激活层`act`\n",
    "        output = act(preact)\n",
    "        # 添加激活后输出的分布情况到`summary`\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造深度神经网络\n",
    "def DNN(x, output_depths, scope='DNN_with_sums', reuse=None):\n",
    "    with tf.name_scope(scope):\n",
    "        net = x\n",
    "        for i, output_depth in enumerate(output_depths):\n",
    "            net = hidden_layer(net, output_depth, scope='hidden%d'%(i+1),reuse=reuse)\n",
    "        # 最后有一个分类层\n",
    "        net = hidden_layer(net, 10, scope='classification',\n",
    "                           act=tf.identity, reuse=reuse)\n",
    "        return net\n",
    "    \n",
    "dnn_with_sums = DNN(input_ph, [400,200,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重新定义`loss`, `acc`, `train_op`\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.losses.softmax_cross_entropy(logits=dnn_with_sums,\n",
    "                                           onehot_labels=label_ph)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn_with_sums,axis=-1),\n",
    "                              tf.argmax(label_ph,axis=-1)),dtype=tf.float32))\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    lr = 1e-2\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (可选)融合summary\n",
    "- 我们可以把前面定义的所有summary都融合成一个summary<br>\n",
    "`merged = tf.summary.merge_all`\n",
    "- 也可以只是融合某些summary<br>\n",
    "`merged = tf.summary.merge([loss_sum, image_sum])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出summary\n",
    "summary是需要导出到外部文件的\n",
    "- 首先定义一个文件读写器<br>\n",
    "`summary_weiter = tf.summary.FileWriter('graph/summaries', sess.graph)`\n",
    "- 然后在训练的过程中, 在你希望的时候运行一次merged或者是你之前自己定义的某个通过summary定义的op<br>\n",
    "`summaries = sess.run(merged, feed_dict={...})`\n",
    "- 然后将这个summaries写入到summari_writer内,注意step表示你当前训练的步数, 当然你也可以设定为其他你想要用的数值<br>\n",
    "`summary_writer.add_summary(summaries, step)`\n",
    "- 最后关闭文件读写器<br>\n",
    "`summary_writer.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000   Train_loss 0.3608   Train_acc 0.891\n",
      "             Test_loss 0.3398    Test_acc 0.922\n",
      "\n",
      "Epoch 2000   Train_loss 0.1051   Train_acc 0.969\n",
      "             Test_loss 0.0975    Test_acc 0.969\n",
      "\n",
      "Epoch 3000   Train_loss 0.1305   Train_acc 0.984\n",
      "             Test_loss 0.2682    Test_acc 0.938\n",
      "\n",
      "Epoch 4000   Train_loss 0.1280   Train_acc 0.953\n",
      "             Test_loss 0.1472    Test_acc 0.953\n",
      "\n",
      "Epoch 5000   Train_loss 0.1943   Train_acc 0.969\n",
      "             Test_loss 0.1888    Test_acc 0.953\n",
      "\n",
      "Epoch 6000   Train_loss 0.0442   Train_acc 1.000\n",
      "             Test_loss 0.0521    Test_acc 1.000\n",
      "\n",
      "Epoch 7000   Train_loss 0.0711   Train_acc 0.984\n",
      "             Test_loss 0.0906    Test_acc 0.984\n",
      "\n",
      "Epoch 8000   Train_loss 0.0992   Train_acc 0.984\n",
      "             Test_loss 0.2433    Test_acc 0.938\n",
      "\n",
      "Epoch 9000   Train_loss 0.0317   Train_acc 1.000\n",
      "             Test_loss 0.1097    Test_acc 0.969\n",
      "\n",
      "Epoch 10000   Train_loss 0.0521   Train_acc 0.984\n",
      "             Test_loss 0.1738    Test_acc 0.938\n",
      "\n",
      "Epoch 11000   Train_loss 0.1070   Train_acc 0.984\n",
      "             Test_loss 0.0524    Test_acc 0.984\n",
      "\n",
      "Epoch 12000   Train_loss 0.0616   Train_acc 0.969\n",
      "             Test_loss 0.1654    Test_acc 0.953\n",
      "\n",
      "Epoch 13000   Train_loss 0.0510   Train_acc 1.000\n",
      "             Test_loss 0.1829    Test_acc 0.953\n",
      "\n",
      "Epoch 14000   Train_loss 0.0511   Train_acc 0.984\n",
      "             Test_loss 0.0535    Test_acc 0.984\n",
      "\n",
      "Epoch 15000   Train_loss 0.0330   Train_acc 1.000\n",
      "             Test_loss 0.0614    Test_acc 0.969\n",
      "\n",
      "Epoch 16000   Train_loss 0.0393   Train_acc 1.000\n",
      "             Test_loss 0.0745    Test_acc 0.984\n",
      "\n",
      "Epoch 17000   Train_loss 0.0112   Train_acc 1.000\n",
      "             Test_loss 0.0412    Test_acc 0.984\n",
      "\n",
      "Epoch 18000   Train_loss 0.0220   Train_acc 1.000\n",
      "             Test_loss 0.0540    Test_acc 0.984\n",
      "\n",
      "Epoch 19000   Train_loss 0.0700   Train_acc 0.984\n",
      "             Test_loss 0.0558    Test_acc 0.969\n",
      "\n",
      "Epoch 20000   Train_loss 0.0477   Train_acc 0.984\n",
      "             Test_loss 0.1038    Test_acc 0.953\n",
      "\n",
      "Train Done!\n"
     ]
    }
   ],
   "source": [
    "train_writer = tf.summary.FileWriter('graph/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('graph/test', sess.graph)\n",
    "\n",
    "batch_size = 64\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size=batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph:images, label_ph:labels})\n",
    "    if (i+1)%1000==0:\n",
    "        test_images, test_labels = test_set.next_batch(batch_size=batch_size)\n",
    "        # 获取`train`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_train, loss_train, acc_train = sess.run([merged, loss, accuracy], \n",
    "                    feed_dict={input_ph:images, label_ph:labels})\n",
    "        # 将`train`的`summaries`写入到`train_writer`中\n",
    "        train_writer.add_summary(sum_train,i)\n",
    "        # 获取`test`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_test, loss_test, acc_test = sess.run([merged, loss, accuracy], \n",
    "                    feed_dict={input_ph:test_images, label_ph:test_labels})\n",
    "        # 将`test`的`summaries`写入到`test_writer`中\n",
    "        test_writer.add_summary(sum_test,i)\n",
    "        print('Epoch %d   Train_loss %.4f   Train_acc %.3f\\n             Test_loss %.4f    Test_acc %.3f\\n'%(i+1,\n",
    "                          loss_train, acc_train, loss_test, acc_test))\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "print('Train Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.0458   Acurracy 0.987\n",
      "Test Loss 0.0949   Acurracy 0.973\n"
     ]
    }
   ],
   "source": [
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "batch_size = 64\n",
    "for i in range(train_set.num_examples // batch_size):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    loss_train, acc_train = sess.run([loss,accuracy],\n",
    "                                     feed_dict={input_ph:images, label_ph:labels})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "    \n",
    "print('Train Loss {:.4f}   Acurracy {:.3f}'.format((np.array(train_loss).mean()),\n",
    "                                         np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "batch_size = 64\n",
    "for i in range(test_set.num_examples // batch_size):\n",
    "    images, labels = test_set.next_batch(batch_size)\n",
    "    loss_test, acc_test = sess.run([loss,accuracy],\n",
    "                                     feed_dict={input_ph:images, label_ph:labels})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "    \n",
    "print('Test Loss {:.4f}   Acurracy {:.3f}'.format((np.array(test_loss).mean()),\n",
    "                                         np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开Tensorboard\n",
    "在之前对计算图可视化的时候, 我们用tensorboard --logdir=.命令打开过Tensorboard显示当前目录下`.但Tensorboard支持打开多个目录下的.events`文件, 方便我们对比不同模型或者训练和测试之间的差别在test_summary目录中输入以下命令<br>\n",
    "`$ tensorboard --logdir=train:train/,test:test/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
