{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:233: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:82: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:129: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:135: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.layers import conv, max_pool, fc\n",
    "from utils.learning import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_block(inputs, num_convs, out_depth, scope='vgg_block', reuse=None):\n",
    "    \"\"\"构建vgg_block.\n",
    "    \n",
    "    一个 vgg_block 由`num_convs`个卷积层和一个最大值池化层构成.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 这一个block里卷积层的个数\n",
    "        out_depth: 每一个卷积层的卷积核个数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    int_depth = inputs.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i in range(num_convs):\n",
    "            net = conv(net, ksize=[3,3], out_depth=out_depth, strides=[1,1],\n",
    "                       padding='SAME', scope='conv%d'%i, reuse=reuse)\n",
    "            net = max_pool(net, [2,2], [2,2], name='pool')\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后我们把很多个不同的`vgg_block`堆叠在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_stack(inputs, num_convs, out_depths, scope='vgg_stack', reuse=None):\n",
    "    \"\"\"构建vgg_stack.\n",
    "    \n",
    "    一个 vgg_stack 将若干个不同的`vgg_block`进行`stack`(堆叠)\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个block里卷积层的个数, 列表. 如`[1, 2, 3]`\n",
    "        out_depths: 每一个block的卷积核个数, 列表, 如`[64, 128, 256]`\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i, (n, d) in enumerate(zip(num_convs, out_depths)):\n",
    "            net = vgg_block(inputs=net, num_convs=n, out_depth=d, scope='vgg_block%d'%i)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(inputs, num_convs, out_depths, num_outputs, scope='vgg', reuse=None):\n",
    "    \"\"\"构建vgg.\n",
    "    \n",
    "    一个 vgg 先经过`vgg_stack`后再连接两个全连接层.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个 vgg_block 的卷积层的个数\n",
    "        out_depths: 每一个 vgg_block 卷积核个数\n",
    "        num_outputs: 最后输出向量的维数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = vgg_stack(inputs, num_convs, out_depths)\n",
    "        with tf.variable_scope('classification'):\n",
    "            net = tf.layers.flatten(net)\n",
    "            net = fc(net, 100, scope='fc1')\n",
    "            net = fc(net, num_outputs, act=tf.identity, scope='classification')\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_out = vgg(inputs=train_imgs, num_convs=(1, 1, 2, 2, 2), \n",
    "                out_depths=(64, 128, 256, 512, 512), \n",
    "                num_outputs=10)\n",
    "# 复用上面的参数\n",
    "val_out = vgg(inputs=val_imgs, num_convs=(1, 1, 2, 2, 2), \n",
    "                out_depths=(64, 128, 256, 512, 512), \n",
    "                num_outputs=10, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), dtype=tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = optimizer.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\learning.py:39: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "[train]: step 0 loss = 2.3167 acc = 0.0781 (0.0088 / batch)\n",
      "[val]: step 0 loss = 2.3165 acc = 0.1562\n",
      "[train]: step 1000 loss = 1.1907 acc = 0.5156 (0.1211 / batch)\n",
      "[train]: step 2000 loss = 0.5664 acc = 0.8125 (0.1264 / batch)\n",
      "[train]: step 3000 loss = 0.7340 acc = 0.7500 (0.1302 / batch)\n",
      "[train]: step 4000 loss = 0.8368 acc = 0.6875 (0.1321 / batch)\n",
      "[val]: step 4000 loss = 0.7107 acc = 0.7188\n",
      "[train]: step 5000 loss = 0.3022 acc = 0.8906 (0.1327 / batch)\n",
      "[train]: step 6000 loss = 0.2720 acc = 0.9219 (0.1299 / batch)\n",
      "[train]: step 7000 loss = 0.1600 acc = 0.9531 (0.1274 / batch)\n",
      "[train]: step 8000 loss = 0.2381 acc = 0.9062 (0.1321 / batch)\n",
      "[val]: step 8000 loss = 0.6860 acc = 0.7969\n",
      "[train]: step 9000 loss = 0.1761 acc = 0.9375 (0.1312 / batch)\n",
      "[train]: step 10000 loss = 0.1726 acc = 0.9219 (0.1305 / batch)\n",
      "[train]: step 11000 loss = 0.0781 acc = 0.9844 (0.1289 / batch)\n",
      "[train]: step 12000 loss = 0.0894 acc = 0.9844 (0.1346 / batch)\n",
      "[val]: step 12000 loss = 1.4966 acc = 0.6875\n",
      "[train]: step 13000 loss = 0.0480 acc = 0.9844 (0.1353 / batch)\n",
      "[train]: step 14000 loss = 0.1042 acc = 0.9531 (0.1376 / batch)\n",
      "[train]: step 15000 loss = 0.0228 acc = 1.0000 (0.1311 / batch)\n",
      "[train]: step 16000 loss = 0.0026 acc = 1.0000 (0.1305 / batch)\n",
      "[val]: step 16000 loss = 1.6437 acc = 0.7031\n",
      "[train]: step 17000 loss = 0.1177 acc = 0.9844 (0.1306 / batch)\n",
      "[train]: step 18000 loss = 0.0142 acc = 1.0000 (0.1324 / batch)\n",
      "[train]: step 19000 loss = 0.0747 acc = 0.9688 (0.1339 / batch)\n",
      "[train]: step 20000 loss = 0.0231 acc = 0.9844 (0.1343 / batch)\n",
      "[val]: step 20000 loss = 1.6456 acc = 0.7656\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.0225 acc = 0.9926\n",
      "[VAL]: loss = 1.3825 acc = 0.7713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
