{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 高层API\n",
    "从我们构建AlexNet模型可以看出,tensorflow在定义很多神经网络的基本单元的时候是比较复杂和冗长的.因此大部分时候人们都倾向于使用对tensorflow底层代码进行封装的高层api，比如keras,slim,tflearn,skflow等等.在这里,我们来分别使用keras和slim这两个非常流行的高层api尝试构造AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    ">- 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）\n",
    ">- 支持CNN和RNN，或二者的结合\n",
    ">- 无缝CPU和GPU切换\n",
    "\n",
    ">Keras的设计原则是\n",
    "\n",
    ">- 用户友好：Keras是为人类而不是天顶星人设计的API。用户的使用体验始终是我们考虑的首要和中心内容。Keras遵循减少认知困难的最佳实践：Keras提供一致而简洁的API， 能够极大减少一般应用下用户的工作量，同时，Keras提供清晰和具有实践意义的bug反馈。\n",
    ">- 模块性：模型可理解为一个层的序列或数据的运算图，完全可配置的模块可以用最少的代价自由组合在一起。具体而言，网络层、损失函数、优化器、初始化策略、激活函数、正则化方法都是独立的模块，你可以使用它们来构建自己的模型。\n",
    ">- 易扩展性：添加新模块超级容易，只需要仿照现有的模块编写新的类或函数即可。创建新模块的便利性使得Keras更适合于先进的研究工作。\n",
    "与Python协作：Keras没有单独的模型配置文件类型（作为对比，caffe有），模型由python代码描述，使其更紧凑和更易debug，并提供了扩展的便利性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from utils import cifar10_input\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "Keras自带了cifar10数据集,并且自动分成了训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float')\n",
    "x_test = x_test.astype('float')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras的数据不需要转化成tensorflow下的tensor,可以直接作为网络的输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'numpy.ndarray'>, <class 'numpy.ndarray'>)\n",
      "((50000, 32, 32, 3), (50000, 1))\n",
      "((10000, 32, 32, 3), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "print((type(x_train), type(y_train)))\n",
    "print((x_train.shape, y_train.shape))\n",
    "print((x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建Keras模型\n",
    "\n",
    "## Keras网络层\n",
    "Keras为了方便用户搭建神经网络模型, 把很多常用的层, 比如`Conv2d`, `MaxPooling2d`,封装起来, 使得输入更加简单明了.\n",
    "\n",
    "## Keras模型\n",
    "Keras提供`Sequential`和`Model`两种模型的构建方法, 使用他们搭建模型就像搭积木一样非常直观简单.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从keras中导入我们需要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后添加第一个卷积层, 卷积核为5x5x64, 步长为1x1, 激活函数是relu\n",
    "- 注意, 添加第一个层的时候需要注明输入的形状是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), \n",
    "                 input_shape=(32,32,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 添加第二个池化层, 核为3x3, 步长为2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第三个卷积层5x5x64,步长为1x1,激活是relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第四层是核大小3x3, 步长为2x2的池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将矩阵摊平成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 全连接层, 输出为384维向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=384, activation='relu'))\n",
    "model.add(Dense(units=192, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经完成了模型的搭建,.\n",
    "\n",
    "`keras`还提供了很多方法帮助我们理解模型\n",
    "- `model.summary()`\n",
    "- `plot_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 384)               393600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 192)               73920     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1930      \n",
      "=================================================================\n",
      "Total params: 576,778\n",
      "Trainable params: 576,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='img/keras_alexnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 217.00 629.00\" width=\"217pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-625 213,-625 213,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 3197610837832 -->\n",
       "<g class=\"node\" id=\"node1\"><title>3197610837832</title>\n",
       "<polygon fill=\"none\" points=\"16.5,-584.5 16.5,-620.5 192.5,-620.5 192.5,-584.5 16.5,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-598.8\">conv2d_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 3197610837776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>3197610837776</title>\n",
       "<polygon fill=\"none\" points=\"41,-511.5 41,-547.5 168,-547.5 168,-511.5 41,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-525.8\">conv2d_1: Conv2D</text>\n",
       "</g>\n",
       "<!-- 3197610837832&#45;&gt;3197610837776 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>3197610837832-&gt;3197610837776</title>\n",
       "<path d=\"M104.5,-584.313C104.5,-576.289 104.5,-566.547 104.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-557.529 104.5,-547.529 101,-557.529 108,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197610837272 -->\n",
       "<g class=\"node\" id=\"node3\"><title>3197610837272</title>\n",
       "<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 209,-474.5 209,-438.5 0,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-452.8\">max_pooling2d_1: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 3197610837776&#45;&gt;3197610837272 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>3197610837776-&gt;3197610837272</title>\n",
       "<path d=\"M104.5,-511.313C104.5,-503.289 104.5,-493.547 104.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-484.529 104.5,-474.529 101,-484.529 108,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197610838448 -->\n",
       "<g class=\"node\" id=\"node4\"><title>3197610838448</title>\n",
       "<polygon fill=\"none\" points=\"41,-365.5 41,-401.5 168,-401.5 168,-365.5 41,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-379.8\">conv2d_2: Conv2D</text>\n",
       "</g>\n",
       "<!-- 3197610837272&#45;&gt;3197610838448 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>3197610837272-&gt;3197610838448</title>\n",
       "<path d=\"M104.5,-438.313C104.5,-430.289 104.5,-420.547 104.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-411.529 104.5,-401.529 101,-411.529 108,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197610836712 -->\n",
       "<g class=\"node\" id=\"node5\"><title>3197610836712</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 209,-328.5 209,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-306.8\">max_pooling2d_2: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 3197610838448&#45;&gt;3197610836712 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>3197610838448-&gt;3197610836712</title>\n",
       "<path d=\"M104.5,-365.313C104.5,-357.289 104.5,-347.547 104.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-338.529 104.5,-328.529 101,-338.529 108,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197610836656 -->\n",
       "<g class=\"node\" id=\"node6\"><title>3197610836656</title>\n",
       "<polygon fill=\"none\" points=\"50,-219.5 50,-255.5 159,-255.5 159,-219.5 50,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-233.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 3197610836712&#45;&gt;3197610836656 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>3197610836712-&gt;3197610836656</title>\n",
       "<path d=\"M104.5,-292.313C104.5,-284.289 104.5,-274.547 104.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-265.529 104.5,-255.529 101,-265.529 108,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197611309544 -->\n",
       "<g class=\"node\" id=\"node7\"><title>3197611309544</title>\n",
       "<polygon fill=\"none\" points=\"52.5,-146.5 52.5,-182.5 156.5,-182.5 156.5,-146.5 52.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 3197610836656&#45;&gt;3197611309544 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>3197610836656-&gt;3197611309544</title>\n",
       "<path d=\"M104.5,-219.313C104.5,-211.289 104.5,-201.547 104.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-192.529 104.5,-182.529 101,-192.529 108,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197611309656 -->\n",
       "<g class=\"node\" id=\"node8\"><title>3197611309656</title>\n",
       "<polygon fill=\"none\" points=\"52.5,-73.5 52.5,-109.5 156.5,-109.5 156.5,-73.5 52.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 3197611309544&#45;&gt;3197611309656 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>3197611309544-&gt;3197611309656</title>\n",
       "<path d=\"M104.5,-146.313C104.5,-138.289 104.5,-128.547 104.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-119.529 104.5,-109.529 101,-119.529 108,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3197611310048 -->\n",
       "<g class=\"node\" id=\"node9\"><title>3197611310048</title>\n",
       "<polygon fill=\"none\" points=\"52.5,-0.5 52.5,-36.5 156.5,-36.5 156.5,-0.5 52.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 3197611309656&#45;&gt;3197611310048 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>3197611309656-&gt;3197611310048</title>\n",
       "<path d=\"M104.5,-73.3129C104.5,-65.2895 104.5,-55.5475 104.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108,-46.5288 104.5,-36.5288 101,-46.5289 108,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可视化模型\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型编译\n",
    "模型构建完成之后, 我们需要用compile来配置训练过程\n",
    "\n",
    "model.compile()接受三个参数:\n",
    "- optimizer: 优化方法, 有\"sgd\",\"rmsprop\",\"adgrad\"等这样的字符串, 也可以是keras.Optimizers对象\n",
    "- loss: 损失函数, 有categorical_crossentropy, mes等这样的字符串, 也可以是函数形式\n",
    "- metrics: 评价函数, 如['accuracy'], 也支持自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "# 将标签转换one_hot形式\n",
    "labels = keras.utils.to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd, \n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "keras训练模型非常简单, 用一个fit函数就可以搞定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 1.6782 - acc: 0.3854\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 15s 307us/step - loss: 1.2516 - acc: 0.5541\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 15s 307us/step - loss: 1.0722 - acc: 0.6232\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 16s 318us/step - loss: 0.9375 - acc: 0.6720\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 0.8294 - acc: 0.7074\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.7405 - acc: 0.7386\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 17s 347us/step - loss: 0.6633 - acc: 0.7667\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 16s 323us/step - loss: 0.5912 - acc: 0.7899\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 0.5206 - acc: 0.8163\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.4566 - acc: 0.8360\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 16s 324us/step - loss: 0.3971 - acc: 0.8588\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 16s 318us/step - loss: 0.3386 - acc: 0.8788\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 16s 323us/step - loss: 0.3095 - acc: 0.8893\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 0.2791 - acc: 0.9008\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 16s 318us/step - loss: 0.2545 - acc: 0.9097\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.2139 - acc: 0.9238\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 0.1919 - acc: 0.9318\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.1931 - acc: 0.9315\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 16s 330us/step - loss: 0.1822 - acc: 0.9363\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 16s 327us/step - loss: 0.1589 - acc: 0.9448\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 0.1595 - acc: 0.9447\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 0.1467 - acc: 0.9485\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 0.1339 - acc: 0.9538\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.1204 - acc: 0.9598\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 0.1367 - acc: 0.9546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e8809146a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=labels,\n",
    "          epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价\n",
    "keras的评价也非常简单, 用一个evaluate就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 7s 134us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11329674496173858, 0.96232]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 135us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8118828453063964, 0.6968]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_test = keras.utils.to_categorical(y_test)\n",
    "model.evaluate(x_test,onehot_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-slim\n",
    "TF-Slim是一个可以在tensorflow中实现构建模型,训练模型,评估模型的轻量级代码库. 可以和原生的tensorflow或者其他例如tf.contrib.learn这样的框架自由组合."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:233: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\software\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:82: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:129: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From E:\\project\\jupyter\\天池 深度学习理论与实战\\第四章 卷积神经网络\\utils\\cifar10_input.py:135: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "data_dir = 'E:/data/cifar10_data/cifar-10-batches-bin'\n",
    "\n",
    "train_imgs, train_labels = cifar10_input.inputs(eval_data=False,\n",
    "                                                data_dir=data_dir,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "val_imgs, val_labels = cifar10_input.inputs(eval_data=True,\n",
    "                                            data_dir=data_dir,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "#### slim中的高级层\n",
    "slim也对tensorflow的底层API进行了层的封装, 像Keras一样, 它也具有\n",
    "- slim.conv2d\n",
    "- slim.max_pool2d\n",
    "- slim.flatten\n",
    "- slim.fully_connected\n",
    "- slim.batch_norm\n",
    "\n",
    "等等高级层, 这些接口也是用户友好的, 例如slim.conv2d\n",
    "\n",
    "第一个参数是输入, 第二个是输出大小, 也就是卷积核的个数, 第三个是卷积核大小, 是一个长度为2的向量, 后面还有很多默认参数,帮助我们快速定义一个卷积层,  可以说是非常简单了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arg_scope\n",
    "然后给大家介绍一个slim中独有也非常好用的一个功能: `slim.arg_scope()`\n",
    "\n",
    "我们知道, 在构建模型的时候会遇到很多相同的参数, 比如说很多卷积层或者池化层的补零策略都是`\"VALID\"`或者`\"SAME\"`, 很多变量的初始化函数都是tf.truncated_normal_initializer或者tf.constant_initializer, 如果全部手动写就会显得非常麻烦. \n",
    "\n",
    "这个时候, 可以通过python的with语句和slim.arg_scope()构成一个参数域, 在这个域下一次性定义好所有函数的一些默认参数, 就会非常方便了\n",
    "- - -\n",
    "使用arg_scope分为两个步骤\n",
    "- 定义你要对哪些函数使用默认参数\n",
    "- 定义你要使用的默认参数的具体值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义AlexNet默认参数域\n",
    "def AlexNet_arg_scope():\n",
    "    # 首先我们定义卷积和全连接层的参数域, 他们都用`tf.nn.relu`作为激活函数\n",
    "    # 都用`tf.truncated_normal`作为权重的初始化函数\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       weights_initializer=tf.truncated_normal_initializer(stddev=1e-2)):\n",
    "        # 在参数域内部继续定义参数域, 这里, 我们注意到在`AlexNet`中\n",
    "        # 卷积和池化的补零策略都是`VALID`\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                           padding='VALID') as sc:\n",
    "            return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义alexnet模型\n",
    "def AlexNet(inputs, reuse=None):\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        net = slim.conv2d(inputs, 64, [5, 5], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [3, 3], scope='pool1')\n",
    "        net = slim.conv2d(net, 64, [5, 5], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [3, 3], scope='pool2')\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 384, scope='fc3')\n",
    "        net = slim.fully_connected(net, 192, scope='fc4')\n",
    "        net = slim.fully_connected(net, 10, activation_fn=None, scope='classification')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope(AlexNet_arg_scope()):\n",
    "    train_out = AlexNet(train_imgs)\n",
    "    val_out = AlexNet(val_imgs, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slim 统计模型\n",
    "Slim也有类似Keras的模型统计功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "AlexNet/conv1/weights:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "AlexNet/conv1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "AlexNet/conv2/weights:0 (float32_ref 5x5x64x64) [102400, bytes: 409600]\n",
      "AlexNet/conv2/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "AlexNet/fc3/weights:0 (float32_ref 1024x384) [393216, bytes: 1572864]\n",
      "AlexNet/fc3/biases:0 (float32_ref 384) [384, bytes: 1536]\n",
      "AlexNet/fc4/weights:0 (float32_ref 384x192) [73728, bytes: 294912]\n",
      "AlexNet/fc4/biases:0 (float32_ref 192) [192, bytes: 768]\n",
      "AlexNet/classification/weights:0 (float32_ref 192x10) [1920, bytes: 7680]\n",
      "AlexNet/classification/biases:0 (float32_ref 10) [10, bytes: 40]\n",
      "Total size of variables: 576778\n",
      "Total bytes of variables: 2307112\n"
     ]
    }
   ],
   "source": [
    "vars_size, vars_bytes = slim.model_analyzer.analyze_vars(variables=tf.model_variables(), print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "slim提供了简化训练的一些方法, 但是封装太多不够灵活, 如果想探究的话可以参考tf-slim的[官方说明](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim).\n",
    "\n",
    "在这里我们还是回到之前用tensorflow定义好的训练函数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "BATCH_SIZE=64\n",
    "def train(train_op, train_loss, train_acc, \n",
    "          val_loss, val_acc, \n",
    "          max_step, batch_size=BATCH_SIZE,\n",
    "          train_log_step=1000, val_log_step=4000):\n",
    "    \"\"\"训练函数\n",
    "      Args:\n",
    "        train_op: 训练`op`\n",
    "        train_loss: 训练集计算误差的`op`\n",
    "        train_acc: 训练集计算正确率的`op`\n",
    "        val_loss: 验证集计算误差的`op`\n",
    "        val_acc: 验证集计算正确率的`op`\n",
    "        max_step: 最大迭代步长\n",
    "        batch_sise: 一个批次中样本的个数\n",
    "        train_log_step: 每隔多少步进行一次训练集信息输出\n",
    "        val_log_step: 每隔多少步进行一次验证集信息输出\n",
    "    \n",
    "      Return:\n",
    "        None\n",
    "      \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "        try:\n",
    "            _start = time.time()\n",
    "            for step in range(max_step + 1):\n",
    "                sess.run(train_op)\n",
    "\n",
    "                if step % train_log_step == 0:\n",
    "                    _end = time.time()\n",
    "                    duration = _end - _start\n",
    "\n",
    "                    train_loss_, train_acc_ = sess.run([train_loss, train_acc])\n",
    "\n",
    "                    sec_per_batch = 1.0 * duration / train_log_step\n",
    "\n",
    "                    print('[train]: step %d loss = %.4f acc = %.4f (%.4f / batch)' \\\n",
    "                      % (step, train_loss_, train_acc_, sec_per_batch))\n",
    "\n",
    "                    _start = _end\n",
    "\n",
    "                if step % val_log_step == 0:\n",
    "                    val_loss_, val_acc_ = sess.run([val_loss, val_acc])\n",
    "                    print('[val]: step %d loss = %.4f acc = %.4f' % (step, val_loss_, val_acc_))\n",
    "\n",
    "            print('-------------------------Over all Result-------------------------')\n",
    "\n",
    "            train_loss_, train_acc_ = _evaluation_no_bn(sess, train_loss, train_acc, NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN, batch_size)\n",
    "            print('[TRAIN]: loss = %.4f acc = %.4f' % (train_loss_, train_acc_))\n",
    "\n",
    "            val_loss_, val_acc_ = _evaluation_no_bn(sess, val_loss, val_acc, NUM_EXAMPLES_PER_EPOCH_FOR_EVAL, batch_size)\n",
    "            print('[VAL]: loss = %.4f acc = %.4f' % (val_loss_, val_acc_))\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Epoch Limited. Done!')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "def _evaluation_no_bn(sess, loss_op, acc_op, num_examples, batch_size):\n",
    "    max_steps = num_examples // batch_size\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for _ in range(max_steps):\n",
    "        loss_value, acc_value = sess.run([loss_op, acc_op])\n",
    "        losses.append(loss_value)\n",
    "        accs.append(acc_value)\n",
    "\n",
    "    mean_loss = np.array(losses, dtype=np.float32).mean()\n",
    "    mean_acc = np.array(accs, dtype=np.float32).mean()\n",
    "\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.3025 acc = 0.1562 (0.0069 / batch)\n",
      "[val]: step 0 loss = 2.3026 acc = 0.0469\n",
      "[train]: step 1000 loss = 2.3061 acc = 0.0469 (0.0347 / batch)\n",
      "[train]: step 2000 loss = 1.2406 acc = 0.5312 (0.0360 / batch)\n",
      "[val]: step 2000 loss = 1.4333 acc = 0.4219\n",
      "[train]: step 3000 loss = 1.2226 acc = 0.5312 (0.0374 / batch)\n",
      "[train]: step 4000 loss = 1.0619 acc = 0.6406 (0.0410 / batch)\n",
      "[val]: step 4000 loss = 0.9758 acc = 0.6875\n",
      "[train]: step 5000 loss = 0.4562 acc = 0.8438 (0.0432 / batch)\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.6854 acc = 0.7619\n",
      "[VAL]: loss = 0.8585 acc = 0.7119\n"
     ]
    }
   ],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
